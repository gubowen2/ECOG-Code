{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f211fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f969215",
   "metadata": {},
   "source": [
    "## Step 1: Pool only the data with ground truth ECOG PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(model, original_text):\n",
    "    if (original_text):\n",
    "        validation_with_PS = pd.read_csv(f\"{model} validation result (Valid PS - Original Text).csv\")\n",
    "        test_with_PS = pd.read_csv(f\"{model} test result (Valid PS - Original Text).csv\")\n",
    "        model_dfs = [validation_with_PS, test_with_PS]\n",
    "    else:\n",
    "        validation_no_PS = pd.read_csv(f\"{model} validation result (Valid PS - PS Removed Text).csv\")\n",
    "        test_no_PS = pd.read_csv(f\"{model} test result (Valid PS - PS Removed Text).csv\")\n",
    "        model_dfs = [validation_no_PS, test_no_PS]\n",
    "\n",
    "    # Concatenate the list of DataFrames by rows\n",
    "    model_all = pd.concat(model_dfs, axis=0)\n",
    "    model_all = model_all.reset_index(drop=True)\n",
    "    return model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_all_original = get_all_data(\"CNN\", True)\n",
    "CNN_all_ps_removed = get_all_data(\"CNN\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "LongFormer_all_original = get_all_data(\"LongFormer\", True)\n",
    "LongFormer_all_ps_removed = get_all_data(\"LongFormer\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221ce24",
   "metadata": {},
   "source": [
    "## Step 2: Randomly sample 100 group truth positive and 100 group truth negative text from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1174dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model_all_original, model_all_ps_removed):\n",
    "    # Selecting 100 rows where ground truth is 0 and 100 rows where ground is 1\n",
    "    selected_rows_0 = model_all_original[model_all_original['high_ps'] == 0].sample(n=n_sample // 2, random_state=0)\n",
    "    selected_rows_1 = model_all_original[model_all_original['high_ps'] == 1].sample(n=n_sample // 2, random_state=0)\n",
    "\n",
    "    # Concatenating the two sets of rows\n",
    "    original_samples = pd.concat([selected_rows_0, selected_rows_1])\n",
    "\n",
    "    # Getting the indices of the selected rows\n",
    "    selected_indices = original_samples.index\n",
    "\n",
    "    # Selecting the same rows from CNN_all dataframe\n",
    "    ps_removed_samples = model_all_ps_removed.loc[selected_indices]\n",
    "\n",
    "    original_samples = original_samples.reset_index(drop=True)\n",
    "    ps_removed_samples = ps_removed_samples.reset_index(drop=True)\n",
    "    return original_samples, ps_removed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_original_samples, CNN_ps_removed_samples = generate_samples(CNN_all_original, CNN_all_ps_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a929817",
   "metadata": {},
   "outputs": [],
   "source": [
    "LongFormer_original_samples, LongFormer_ps_removed_samples = generate_samples(LongFormer_all_original, LongFormer_all_ps_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34798d18",
   "metadata": {},
   "source": [
    "## Step 3: Generate sentence removal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"./best_Longformer_model\")\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-large-4096\", num_labels=2)\n",
    "\n",
    "# Load the saved weights into the model\n",
    "model.load_state_dict(torch.load(\"./best_Longformer_model/pytorch_model.bin\"))\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 4096\n",
    "def filter_exceeding_texts(note, tokenizer):\n",
    "    tokens = tokenizer.tokenize(note)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    if num_tokens > MAX_TOKENS:\n",
    "        # Tokenize the note and then convert back to string \n",
    "        # only the last MAX_TOKENS of tokens\n",
    "        filtered_note = tokenizer.convert_tokens_to_string(tokens[-MAX_TOKENS:])\n",
    "        return filtered_note\n",
    "    else:\n",
    "        return note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ce085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    text = str(text)\n",
    "    result = pd.DataFrame()\n",
    "    sentence_id = []\n",
    "    remaining_text = []\n",
    "    removed_sentence = []\n",
    "    sentence_list = text.split(\".\")\n",
    "    for i in range(0, len(sentence_list)):\n",
    "        sentence_id.append(i + 1)\n",
    "        sentence_list_copy = copy.deepcopy(sentence_list)\n",
    "        del sentence_list_copy[i]\n",
    "        remaining_text.append(\".\".join(sentence_list_copy))\n",
    "        removed_sentence.append(sentence_list[i])\n",
    "    result[\"Removed Sentence ID\"] = sentence_id\n",
    "    result[\"Remaining Text\"] = remaining_text\n",
    "    result[\"Removed Sentence\"] = removed_sentence\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_removal_file(model, original_text):\n",
    "    if (original_text):\n",
    "        target_rows = LongFormer_original_samples\n",
    "    else :\n",
    "        target_rows = LongFormer_ps_removed_samples\n",
    "    for i in range(0, len(target_rows)):\n",
    "        if original_text:\n",
    "            text = target_rows.iloc[i][\"text\"]\n",
    "        else:\n",
    "            text = target_rows.iloc[i][\"text_no_ps\"]\n",
    "        if (model == \"LongFormer\"):\n",
    "            text = filter_exceeding_texts(text, tokenizer) # The LongFormer model only sees the last 4096 tokens\n",
    "        result_df = generate_text(text)\n",
    "        truth = target_rows.iloc[i][\"high_ps\"]\n",
    "        prediction = target_rows.iloc[i][\"Prediction\"]\n",
    "        logits_0 = target_rows.iloc[i][\"Logits (Class 0)\"]\n",
    "        logits_1 = target_rows.iloc[i][\"Logits (Class 1)\"]\n",
    "        probability_0 = target_rows.iloc[i][\"Probability (Class 0)\"]\n",
    "        probability_1 = target_rows.iloc[i][\"Probability (Class 1)\"]\n",
    "        result_df.insert(0, \"Original Text\", [text] * len(result_df))\n",
    "        result_df[\"Ground Truth\"] = [truth] * len(result_df)\n",
    "        result_df[\"Original Prediction\"] = [prediction] * len(result_df)\n",
    "        result_df[\"Original Logits (Class 0)\"] = [logits_0] * len(result_df)\n",
    "        result_df[\"Original Logits (Class 1)\"] = [logits_1] * len(result_df)\n",
    "        result_df[\"Original Probability (Class 0)\"] = [probability_0] * len(result_df)\n",
    "        result_df[\"Original Probability (Class 1)\"] = [probability_1] * len(result_df)\n",
    "        if original_text:\n",
    "            result_df.to_csv(f\"{model} (Sentence Removal) ({i + 1}) (Valid PS - Original Text).csv\", index = False)\n",
    "        else:\n",
    "            result_df.to_csv(f\"{model} (Sentence Removal) ({i + 1}) (Valid PS - PS Removed Text).csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence_removal_file(\"CNN\", True)\n",
    "generate_sentence_removal_file(\"CNN\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08520243",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence_removal_file(\"LongFormer\", True)\n",
    "generate_sentence_removal_file(\"LongFormer\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e471d4",
   "metadata": {},
   "source": [
    "## Step 4: Run model inference on sentence removal files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488150d",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffa4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, concatenate, Average\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, TimeDistributed, GRU, Bidirectional, Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb500c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer if already trained\n",
    "with open('notes_tokenizer_ps_find.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_model(target):\n",
    "    vocab_size = 10000\n",
    "    embedding_dims = 256\n",
    "    filters = 250\n",
    "    kernel_size = 3\n",
    "    epochs = 2\n",
    "    hidden_dims = 128\n",
    "    max_note_length=2000\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "    # make model\n",
    "    text_input = Input(shape=(max_note_length,), dtype='float32')\n",
    "\n",
    "    text_embed = Embedding(vocab_size, embedding_dims, input_length=max_note_length, mask_zero=False)(text_input)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=500, kernel_size=kernel_size, strides=1, padding='valid')(text_embed)\n",
    "    x = GlobalMaxPooling1D()(cnn1)\n",
    "\n",
    "    hidden = Dense(hidden_dims)(x)\n",
    "    hidden = Activation('relu')(hidden)\n",
    "\n",
    "    output = Dense(1, activation='linear')(hidden)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=output)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_simple_model('ps_high')\n",
    "model.load_weights('ps_high'+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11db2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logits_and_probability(logits_list):\n",
    "    logits_list = [logit[0] for logit in logits_list]\n",
    "    # Logits\n",
    "    logit_class_1 = logits_list\n",
    "    logit_class_0 = [-logit for logit in logits_list]\n",
    "\n",
    "    # Probabilities\n",
    "    probability_class_1 = [tf.math.sigmoid(logit).numpy() for logit in logits_list]\n",
    "    probability_class_0 = [1 - tf.math.sigmoid(logit).numpy() for logit in logits_list]\n",
    "    \n",
    "    prediction = [1 if a > b else 0 for a, b in zip(probability_class_1, probability_class_0)]\n",
    "    \n",
    "    return logit_class_1, logit_class_0, probability_class_1, probability_class_0, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a712cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(n_sample, original_text):\n",
    "    for i in range(1, n_sample + 1):\n",
    "        if original_text:\n",
    "            file_name =  f\"CNN (Sentence Removal) ({i}) (Valid PS - Original Text).csv\"\n",
    "        else:\n",
    "            file_name =  f\"CNN (Sentence Removal) ({i}) (Valid PS - PS Removed Text).csv\"\n",
    "        data = pd.read_csv(file_name)\n",
    "        text_list = data[\"Remaining Text\"].tolist()\n",
    "        text_list = [\"\" if type(text) != str else text for text in text_list]\n",
    "        vocab_size = 10000\n",
    "        max_note_length = 2000\n",
    "\n",
    "        input_text = sequence.pad_sequences(tokenizer.texts_to_sequences([str(x) for x in text_list]), maxlen=max_note_length, padding='post')\n",
    "        logits_list = model.predict(input_text)\n",
    "\n",
    "        logit_class_1, logit_class_0, probability_class_1, probability_class_0, prediction = generate_logits_and_probability(logits_list)\n",
    "        data[\"Prediction\"] = prediction\n",
    "        data[\"Logits (Class 0)\"] = logit_class_0\n",
    "        data[\"Logits (Class 1)\"] = logit_class_1\n",
    "        data[\"Probability (Class 0)\"] = probability_class_0\n",
    "        data[\"Probability (Class 1)\"] = probability_class_1\n",
    "        data.to_csv(file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15095506",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(n_sample, True)\n",
    "run_inference(n_sample, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bfed6",
   "metadata": {},
   "source": [
    "#### LongFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913cba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import itertools\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436cc732",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8198cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"./best_Longformer_model\")\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-large-4096\", num_labels=2)\n",
    "\n",
    "# Load the saved weights into the model\n",
    "model.load_state_dict(torch.load(\"./best_Longformer_model/pytorch_model.bin\"))\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn_if_truncated(texts, max_length):\n",
    "    for text in texts:\n",
    "        if len(tokenizer.tokenize(text)) > max_length:\n",
    "            print(f\"Warning: Text with length {len(tokenizer.tokenize(text))} is truncated to {max_length} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 4096\n",
    "def filter_exceeding_texts(notes, tokenizer):\n",
    "    filtered_notes = []\n",
    "    \n",
    "    for note in notes:\n",
    "        tokens = tokenizer.tokenize(note)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        if num_tokens > MAX_TOKENS:\n",
    "            # Tokenize the note and then convert back to string \n",
    "            # only the last MAX_TOKENS of tokens\n",
    "            filtered_note = tokenizer.convert_tokens_to_string(tokens[-MAX_TOKENS:])\n",
    "            filtered_notes.append(filtered_note)\n",
    "        else:\n",
    "            filtered_notes.append(note)\n",
    "\n",
    "    return filtered_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 4096\n",
    "def encode_data(texts, max_length=MAX_TOKENS):\n",
    "    warn_if_truncated(texts, max_length)\n",
    "    encoded_data = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81691694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"Convert logits to probabilities.\"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / exp_logits.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10474a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(n_sample, original_text):\n",
    "    for i in range(1, n_sample + 1):\n",
    "        if original_text:\n",
    "            file_name =  f\"LongFormer (Sentence Removal) ({i}) (Valid PS - Original Text).csv\"\n",
    "        else:\n",
    "            file_name =  f\"LongFormer (Sentence Removal) ({i}) (Valid PS - PS Removed Text).csv\"\n",
    "        data = pd.read_csv(file_name)\n",
    "        text_list = data[\"Remaining Text\"].tolist()\n",
    "        text_list = [\"\" if type(text) != str else text for text in text_list]\n",
    "        text_list = filter_exceeding_texts(text_list, tokenizer)\n",
    "\n",
    "        input_ids, attention_masks = encode_data(text_list)\n",
    "\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "        batch_size = 64\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize tqdm for the loop\n",
    "        progress = tqdm(loader, desc=\"Test\", position=0, leave=True)\n",
    "\n",
    "        logits_list = []  # Collect logits for all chunks\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in progress:\n",
    "                inputs, masks = batch[0].to(device), batch[1].to(device)\n",
    "                logits = model(inputs, attention_mask=masks).logits\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                preds.extend(predictions.tolist())\n",
    "\n",
    "                logits_list.extend(logits.tolist())  # Append the logits for this batch\n",
    "\n",
    "        logits_list = np.array(logits_list)\n",
    "        probability = softmax(logits_list)\n",
    "\n",
    "        data[\"Prediction\"] = preds\n",
    "        data[\"Logits (Class 0)\"] = logits_list[:, 0]\n",
    "        data[\"Logits (Class 1)\"] = logits_list[:, 1]\n",
    "        data[\"Probability (Class 0)\"] = probability[:, 0]\n",
    "        data[\"Probability (Class 1)\"] = probability[:, 1]\n",
    "        data.to_csv(file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(n_sample, True)\n",
    "run_inference(n_sample, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f68a4",
   "metadata": {},
   "source": [
    "## Step 5: Calculate the logits and probability difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_logits_and_probability_difference(model, n_sample, original_text):\n",
    "    for i in range(1, n_sample + 1):\n",
    "        if original_text:\n",
    "            file_name =  f\"{model} (Sentence Removal) ({i}) (Valid PS - Original Text).csv\"\n",
    "        else:\n",
    "            file_name =  f\"{model} (Sentence Removal) ({i}) (Valid PS - PS Removed Text).csv\"\n",
    "        data = pd.read_csv(file_name)\n",
    "        data[\"Prediction Difference\"] =  data[\"Prediction\"] - data[\"Original Prediction\"]\n",
    "        data[\"Logits (Class 0) Difference\"] = data[\"Logits (Class 0)\"] - data[\"Original Logits (Class 0)\"]\n",
    "        data[\"Logits (Class 1) Difference\"] = data[\"Logits (Class 1)\"] - data[\"Original Logits (Class 1)\"]\n",
    "        data[\"Probability (Class 0) Difference\"] = data[\"Probability (Class 0)\"] - data[\"Original Probability (Class 0)\"]\n",
    "        data[\"Probability (Class 1) Difference\"] = data[\"Probability (Class 1)\"] - data[\"Original Probability (Class 1)\"]\n",
    "        data[\"Absolute Prediction Difference\"] = (data[\"Prediction\"] - data[\"Original Prediction\"]).abs()\n",
    "        data[\"Absolute Logits (Class 0) Difference\"] = (data[\"Logits (Class 0)\"] - data[\"Original Logits (Class 0)\"]).abs()\n",
    "        data[\"Absolute Logits (Class 1) Difference\"] = (data[\"Logits (Class 1)\"] - data[\"Original Logits (Class 1)\"]).abs()\n",
    "        data[\"Absolute Probability (Class 0) Difference\"] = (data[\"Probability (Class 0)\"] - data[\"Original Probability (Class 0)\"]).abs()\n",
    "        data[\"Absolute Probability (Class 1) Difference\"] = (data[\"Probability (Class 1)\"] - data[\"Original Probability (Class 1)\"]).abs()\n",
    "        data.to_csv(file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7374bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_logits_and_probability_difference(\"CNN\", n_sample, True)\n",
    "add_logits_and_probability_difference(\"CNN\", n_sample, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e863c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_logits_and_probability_difference(\"LongFormer\", n_sample, True)\n",
    "add_logits_and_probability_difference(\"LongFormer\", n_sample, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbad587",
   "metadata": {},
   "source": [
    "## Step 6: Generate the explainability dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60485bb7",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e45b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_CNN_explainability_dataframe(n_sample, original_text):\n",
    "    CNN_explainability = pd.DataFrame()\n",
    "    CNN_text_file_name = []\n",
    "    CNN_ground_truth = []\n",
    "    CNN_original_prediction = []\n",
    "    CNN_original_logits_0 = []\n",
    "    CNN_original_logits_1 = []\n",
    "    CNN_original_probability_0 = []\n",
    "    CNN_original_probability_1 = []\n",
    "    CNN_sum_of_diff = []\n",
    "    CNN_num_of_positive_significant_sentences = []\n",
    "    CNN_num_of_negative_significant_sentences = []\n",
    "    CNN_total_num_of_sentences = []\n",
    "    CNN_most_positive_sentence = []\n",
    "    CNN_most_positive_value = []\n",
    "    CNN_most_negative_sentence = []\n",
    "    CNN_most_negative_value = []\n",
    "\n",
    "    for i in range(1, n_sample + 1):\n",
    "        if (original_text):\n",
    "            file_name =  f\"CNN (Sentence Removal) ({i}) (Valid PS - Original Text).csv\"\n",
    "        else:\n",
    "            file_name =  f\"CNN (Sentence Removal) ({i}) (Valid PS - PS Removed Text).csv\"\n",
    "        data = pd.read_csv(file_name)\n",
    "        CNN_text_file_name.append(file_name.split(\"/\")[-1])\n",
    "        CNN_ground_truth.append(data[\"Ground Truth\"].iloc[0])\n",
    "        CNN_original_prediction.append(data[\"Original Prediction\"].iloc[0])\n",
    "        CNN_original_logits_0.append(data[\"Original Logits (Class 0)\"].iloc[0])\n",
    "        CNN_original_logits_1.append(data[\"Original Logits (Class 1)\"].iloc[0])\n",
    "        CNN_original_probability_0.append(data[\"Original Probability (Class 0)\"].iloc[0])\n",
    "        CNN_original_probability_1.append(data[\"Original Probability (Class 1)\"].iloc[0])\n",
    "        CNN_sum_of_diff.append(data[\"Probability (Class 1) Difference\"].sum())\n",
    "        CNN_num_of_positive_significant_sentences.append(len(data[data['Probability (Class 1) Difference'] > 0.01]))\n",
    "        CNN_num_of_negative_significant_sentences.append(len(data[data['Probability (Class 1) Difference'] < -0.01]))\n",
    "        CNN_total_num_of_sentences.append(len(data))\n",
    "        sorted_df = data.sort_values(by=\"Probability (Class 1) Difference\", ascending=False)\n",
    "        sorted_df.to_csv(file_name, index = False)\n",
    "        CNN_most_positive_sentence.append(sorted_df['Removed Sentence'].iloc[0])\n",
    "        CNN_most_positive_value.append(sorted_df['Probability (Class 1) Difference'].iloc[0])\n",
    "        CNN_most_negative_sentence.append(sorted_df['Removed Sentence'].iloc[-1])\n",
    "        CNN_most_negative_value.append(sorted_df['Probability (Class 1) Difference'].iloc[-1])\n",
    "\n",
    "    CNN_explainability[\"Text file name\"] = CNN_text_file_name\n",
    "    CNN_explainability[\"Ground Truth\"] = CNN_ground_truth\n",
    "    CNN_explainability[\"Original prediction\"] = CNN_original_prediction\n",
    "    CNN_explainability[\"Original Logits (Class 0)\"] = CNN_original_logits_0\n",
    "    CNN_explainability[\"Original Logits (Class 1)\"] = CNN_original_logits_1\n",
    "    CNN_explainability[\"Original Probability (Class 0)\"] = CNN_original_probability_0\n",
    "    CNN_explainability[\"Original Probability (Class 1)\"] = CNN_original_probability_1\n",
    "    CNN_explainability[\"Diff sum\"] = CNN_sum_of_diff\n",
    "    CNN_explainability[\"Num positive significant sentences (>0.01)\"] = CNN_num_of_positive_significant_sentences\n",
    "    CNN_explainability[\"Num negative significant sentences (<-0.01)\"] = CNN_num_of_negative_significant_sentences\n",
    "    CNN_explainability[\"Total sentences\"] = CNN_total_num_of_sentences\n",
    "    CNN_explainability[\"Most positive sentence\"] = CNN_most_positive_sentence\n",
    "    CNN_explainability[\"Most positive value\"] = CNN_most_positive_value\n",
    "    CNN_explainability[\"Most negative sentence\"] = CNN_most_negative_sentence\n",
    "    CNN_explainability[\"Most negative value\"] = CNN_most_negative_value\n",
    "    if original_text:\n",
    "        save_file_name = f\"Explainability Analysis (CNN) (Valid PS - Original Text).csv\"\n",
    "    else:\n",
    "        save_file_name = f\"Explainability Analysis (CNN) (Valid PS - PS Removed Text).csv\"\n",
    "    CNN_explainability.to_csv(save_file_name, index = False)\n",
    "    return CNN_explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_explainability_original_text = generate_CNN_explainability_dataframe(n_sample, True)\n",
    "CNN_explainability_ps_removed_text = generate_CNN_explainability_dataframe(n_sample, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12bf9c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN_explainability_original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_explainability_ps_removed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d20f9",
   "metadata": {},
   "source": [
    "#### LongFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LongFormer_explainability_dataframe(n_sample, original_text):\n",
    "    LongFormer_explainability = pd.DataFrame()\n",
    "    LongFormer_text_file_name = []\n",
    "    LongFormer_ground_truth = []\n",
    "    LongFormer_original_prediction = []\n",
    "    LongFormer_original_logits_0 = []\n",
    "    LongFormer_original_logits_1 = []\n",
    "    LongFormer_original_probability_0 = []\n",
    "    LongFormer_original_probability_1 = []\n",
    "    LongFormer_sum_of_diff = []\n",
    "    LongFormer_num_of_positive_significant_sentences = []\n",
    "    LongFormer_num_of_negative_significant_sentences = []\n",
    "    LongFormer_total_num_of_sentences = []\n",
    "    LongFormer_most_positive_sentence = []\n",
    "    LongFormer_most_positive_value = []\n",
    "    LongFormer_most_negative_sentence = []\n",
    "    LongFormer_most_negative_value = []\n",
    "\n",
    "    for i in range(1, n_sample + 1):\n",
    "        if (original_text):\n",
    "            file_name =  f\"LongFormer (Sentence Removal) ({i}) (Valid PS - Original Text).csv\"\n",
    "        else:\n",
    "            file_name =  f\"LongFormer (Sentence Removal) ({i}) (Valid PS - PS Removed Text).csv\"\n",
    "        data = pd.read_csv(file_name)\n",
    "        LongFormer_text_file_name.append(file_name.split(\"/\")[-1])\n",
    "        LongFormer_ground_truth.append(data[\"Ground Truth\"].iloc[0])\n",
    "        LongFormer_original_prediction.append(data[\"Original Prediction\"].iloc[0])\n",
    "        LongFormer_original_logits_0.append(data[\"Original Logits (Class 0)\"].iloc[0])\n",
    "        LongFormer_original_logits_1.append(data[\"Original Logits (Class 1)\"].iloc[0])\n",
    "        LongFormer_original_probability_0.append(data[\"Original Probability (Class 0)\"].iloc[0])\n",
    "        LongFormer_original_probability_1.append(data[\"Original Probability (Class 1)\"].iloc[0])\n",
    "        LongFormer_sum_of_diff.append(data[\"Probability (Class 1) Difference\"].sum())\n",
    "        LongFormer_num_of_positive_significant_sentences.append(len(data[data['Probability (Class 1) Difference'] > 0.01]))\n",
    "        LongFormer_num_of_negative_significant_sentences.append(len(data[data['Probability (Class 1) Difference'] < -0.01]))\n",
    "        LongFormer_total_num_of_sentences.append(len(data))\n",
    "        sorted_df = data.sort_values(by=\"Probability (Class 1) Difference\", ascending=False)\n",
    "        sorted_df.to_csv(file_name, index = False)\n",
    "        LongFormer_most_positive_sentence.append(sorted_df['Removed Sentence'].iloc[0])\n",
    "        LongFormer_most_positive_value.append(sorted_df['Probability (Class 1) Difference'].iloc[0])\n",
    "        LongFormer_most_negative_sentence.append(sorted_df['Removed Sentence'].iloc[-1])\n",
    "        LongFormer_most_negative_value.append(sorted_df['Probability (Class 1) Difference'].iloc[-1])\n",
    "\n",
    "    LongFormer_explainability[\"Text file name\"] = LongFormer_text_file_name\n",
    "    LongFormer_explainability[\"Ground Truth\"] = LongFormer_ground_truth\n",
    "    LongFormer_explainability[\"Original prediction\"] = LongFormer_original_prediction\n",
    "    LongFormer_explainability[\"Original Logits (Class 0)\"] = LongFormer_original_logits_0\n",
    "    LongFormer_explainability[\"Original Logits (Class 1)\"] = LongFormer_original_logits_1\n",
    "    LongFormer_explainability[\"Original Probability (Class 0)\"] = LongFormer_original_probability_0\n",
    "    LongFormer_explainability[\"Original Probability (Class 1)\"] = LongFormer_original_probability_1\n",
    "    LongFormer_explainability[\"Diff sum\"] = LongFormer_sum_of_diff\n",
    "    LongFormer_explainability[\"Num positive significant sentences (>0.01)\"] = LongFormer_num_of_positive_significant_sentences\n",
    "    LongFormer_explainability[\"Num negative significant sentences (<-0.01)\"] = LongFormer_num_of_negative_significant_sentences\n",
    "    LongFormer_explainability[\"Total sentences\"] = LongFormer_total_num_of_sentences\n",
    "    LongFormer_explainability[\"Most positive sentence\"] = LongFormer_most_positive_sentence\n",
    "    LongFormer_explainability[\"Most positive value\"] = LongFormer_most_positive_value\n",
    "    LongFormer_explainability[\"Most negative sentence\"] = LongFormer_most_negative_sentence\n",
    "    LongFormer_explainability[\"Most negative value\"] = LongFormer_most_negative_value\n",
    "    if original_text:\n",
    "        save_file_name = f\"Explainability Analysis (LongFormer) (Valid PS - Original Text).csv\"\n",
    "    else:\n",
    "        save_file_name = f\"Explainability Analysis (LongFormer) (Valid PS - PS Removed Text).csv\"\n",
    "    LongFormer_explainability.to_csv(save_file_name, index = False)\n",
    "    return LongFormer_explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LongFormer_explainability_original_text = generate_LongFormer_explainability_dataframe(n_sample, True)\n",
    "LongFormer_explainability_ps_removed_text = generate_LongFormer_explainability_dataframe(n_sample, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68edcd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LongFormer_explainability_original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "LongFormer_explainability_ps_removed_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
