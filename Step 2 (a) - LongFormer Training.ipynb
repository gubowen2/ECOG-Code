{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb52953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbc8bd",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ef223",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./best_Longformer_model\"):\n",
    "    os.makedirs(\"./best_Longformer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_clinical_notes (Valid PS).csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e278c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be063c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 4096\n",
    "def filter_exceeding_texts(notes, labels, tokenizer):\n",
    "    filtered_notes = []\n",
    "    filtered_labels = []\n",
    "    \n",
    "    for note, label in zip(notes, labels):\n",
    "        tokens = tokenizer.tokenize(note)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        if num_tokens > MAX_TOKENS:\n",
    "            # Tokenize the note and then convert back to string \n",
    "            # only the last MAX_TOKENS of tokens\n",
    "            filtered_note = tokenizer.convert_tokens_to_string(tokens[-MAX_TOKENS:])\n",
    "            filtered_notes.append(filtered_note)\n",
    "            filtered_labels.append(label)\n",
    "        else:\n",
    "            filtered_notes.append(note)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_notes, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data[\"split\"] == \"train\"]\n",
    "valid_data = data[data[\"split\"] == \"validation\"]\n",
    "test_data = data[data[\"split\"] == \"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes = train_data[\"text_no_ps\"].tolist()\n",
    "train_labels = train_data[\"high_ps\"].tolist()\n",
    "train_notes, train_labels = filter_exceeding_texts(train_notes, train_labels, tokenizer)\n",
    "\n",
    "val_notes = valid_data[\"text_no_ps\"].tolist()\n",
    "val_labels = valid_data[\"high_ps\"].tolist()\n",
    "val_notes, val_labels = filter_exceeding_texts(val_notes, val_labels, tokenizer)\n",
    "\n",
    "test_notes = test_data[\"text_no_ps\"].tolist()\n",
    "test_labels = test_data[\"high_ps\"].tolist()\n",
    "test_notes, test_labels = filter_exceeding_texts(test_notes, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94351af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = [train_labels.count(0), train_labels.count(1)]\n",
    "total_samples = len(train_labels)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = [total_samples / (2.0 * count) for count in class_counts]\n",
    "class_weights = torch.tensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn_if_truncated(texts, max_length):\n",
    "    for text in texts:\n",
    "        if len(tokenizer.tokenize(text)) > max_length:\n",
    "            print(f\"Warning: Text with length {len(tokenizer.tokenize(text))} is truncated to {max_length} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0785df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(texts, labels, max_length=MAX_TOKENS):\n",
    "    warn_if_truncated(texts, max_length)\n",
    "    encoded_data = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = encoded_data['input_ids']\n",
    "    attention_masks = encoded_data['attention_mask']\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    return input_ids, attention_masks, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba50ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks, train_labels = encode_data(train_notes, train_labels)\n",
    "val_input_ids, val_attention_masks, val_labels = encode_data(val_notes, val_labels)\n",
    "test_input_ids, test_attention_masks, test_labels = encode_data(test_notes, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-large-4096\", num_labels=2)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5985d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 100\n",
    "batch_size = 12\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay = 0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataset) * epochs)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Define the unweighted criterion for the training loop\n",
    "unweighted_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define early stopping variables\n",
    "early_stop_counter = 0\n",
    "best_val_f1 = -1.0\n",
    "best_val_loss = 99.0\n",
    "EARLY_STOP_LIMIT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae763f0",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize tqdm for the training loop\n",
    "    train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", position=0, leave=True)\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for batch in train_progress:\n",
    "        inputs, masks, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        logits = model(inputs, attention_mask=masks).logits\n",
    "        #loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss = unweighted_criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        train_preds.extend(preds.tolist())\n",
    "        train_labels.extend(labels.tolist())\n",
    "        acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        train_accuracy += acc\n",
    "\n",
    "        # Update tqdm description with both loss and accuracy\n",
    "        train_progress.set_description(f\"Epoch {epoch+1}/{epochs} - Training Loss: {loss.item():.4f} Acc: {acc:.4f}\")\n",
    "    \n",
    "    train_f1 = f1_score(train_labels, train_preds)\n",
    "    train_accuracy /= len(train_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Average Training Loss: {train_loss/len(train_loader)}, Training F1 Score: {train_f1:.4f}, Training Acc: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Initialize tqdm for the validation loop\n",
    "    val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", position=0, leave=True)\n",
    "    \n",
    "    val_logits_list = []  # Collect logits for all chunks\n",
    "    \n",
    "    # Inside your validation loop\n",
    "    val_accuracy = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress:\n",
    "            inputs, masks, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "            logits = model(inputs, attention_mask=masks).logits\n",
    "            #loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            loss = unweighted_criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_preds.extend(preds.tolist())\n",
    "            val_labels.extend(labels.tolist())\n",
    "            acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "            val_accuracy += acc\n",
    "            \n",
    "            val_logits_list.extend(logits.tolist())  # Append the logits for this batch\n",
    "            \n",
    "            # Update tqdm description\n",
    "            val_progress.set_description(f\"Epoch {epoch+1}/{epochs} - Validation Loss: {loss.item():.4f}\")\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_preds)\n",
    "    val_accuracy /= len(val_loader)\n",
    "    val_loss_avg = val_loss/len(val_loader)\n",
    "    print(f\"Epoch: {epoch+1}, Average Validation Loss: {val_loss_avg}, Validation F1 Score: {val_f1:.4f}, Validation Acc: {val_accuracy:.4f}\")\n",
    "    '''\n",
    "    # Check for early stopping based on loss\n",
    "    if val_loss_avg < best_val_loss:\n",
    "        print(f\"Saving best model associated with the current lowest loss {val_loss_avg:.4f}\")\n",
    "        best_val_loss = val_loss_avg\n",
    "        torch.save(model.module.state_dict(), \"./best_Longformer_model/pytorch_model.bin\")\n",
    "        tokenizer.save_pretrained(\"./best_Longformer_model\")\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= EARLY_STOP_LIMIT:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "    '''\n",
    "\n",
    "    # Check for early stopping based on F1 score\n",
    "    if val_f1 > best_val_f1:\n",
    "        print(f\"Saving best model associated with the current highest F1 score {val_f1:.4f}\")\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.module.state_dict(), \"./best_Longformer_model/pytorch_model.bin\")\n",
    "        tokenizer.save_pretrained(\"./best_Longformer_model\")\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= EARLY_STOP_LIMIT:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58b887",
   "metadata": {},
   "source": [
    "## Validation and test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "\n",
    "# Initialize tqdm for the test loop\n",
    "test_progress = tqdm(test_loader, desc=f\"Testing\", position=0, leave=True)\n",
    "\n",
    "# Inside the test loop\n",
    "test_logits_list = []  # Collect logits for all chunks\n",
    "\n",
    "test_accuracy = 0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_progress:\n",
    "        inputs, masks, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        logits = model(inputs, attention_mask=masks).logits\n",
    "        #loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss = unweighted_criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.extend(preds.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "        acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        test_accuracy += acc\n",
    "        \n",
    "        test_logits_list.extend(logits.tolist())  # Append the logits for this batch\n",
    "\n",
    "        # Update tqdm description\n",
    "        test_progress.set_description(f\"Test Loss: {loss.item():.4f}\")\n",
    "        \n",
    "test_f1 = f1_score(test_labels, test_preds)\n",
    "test_accuracy /= len(test_loader)\n",
    "print(f\"Average Test Loss: {test_loss/len(test_loader)}, Test F1 Score: {test_f1:.4f}, Test Acc: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ad204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to plot model metrics\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "  \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model auc\n",
    "def eval_model(predicted, actual):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import auc\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    print(\"AUC \" + str(roc_auc_score(actual, predicted)))\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    fpr, tpr, threshold = roc_curve(actual, predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    average_precision = average_precision_score(actual, predicted)\n",
    "\n",
    "    print('Average precision score: {0:0.2f}'.format(\n",
    "        average_precision))\n",
    "\n",
    "\n",
    "    # method I: plt\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.title('Receiver Operating Characteristic: ' )\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    #from sklearn.utils.fixes import signature\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(actual, predicted)\n",
    "    \n",
    "    outcome_counts = np.unique(actual, return_counts=True)[1]\n",
    "    prob_outcome = outcome_counts[1] / (outcome_counts[0] + outcome_counts[1])\n",
    "    print('Outcome probability:')\n",
    "    print(prob_outcome)\n",
    "    \n",
    "    plt.plot(recall, precision, color='b')\n",
    "    plt.plot([0,1],[prob_outcome,prob_outcome], 'r--')\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b')\n",
    "\n",
    "    plt.xlabel('Recall (Sensitivity)')\n",
    "    plt.ylabel('Precision (PPV)')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "            average_precision))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # best F1\n",
    "    F1 = 2*((precision*recall)/(precision+recall))\n",
    "    print(\"Best F1 \")\n",
    "    print(max(F1))\n",
    "    \n",
    "    # threshold for best F1\n",
    "    bestF1_thresh = thresholds[np.argmax(F1)]\n",
    "    print(\"Threshold for best F1:\")\n",
    "    print(bestF1_thresh)\n",
    "    pred_outcome_best_f1_thresh = np.where(predicted >= bestF1_thresh,1,0)\n",
    "    print(np.unique(pred_outcome_best_f1_thresh, return_counts=True))\n",
    "    pred_outcome_00_thresh = np.where(predicted >= 0.0,1,0)\n",
    "    \n",
    "    # # predictions\n",
    "    \n",
    "    # # confusion matrix\n",
    "    print(\"Confusion matrix at best F1 thresh:\")\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cnf_matrix = confusion_matrix(actual, pred_outcome_best_f1_thresh)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['No','Yes'],\n",
    "                        title='Confusion matrix, without normalization')\n",
    "    print(\"Metrics at best F1 thresh (specificity is recall for negative class):\")\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(actual, pred_outcome_best_f1_thresh, target_names=['No','Yes']))\n",
    "\n",
    "\n",
    "    print(\"Confusion matrix at 0.0 thresh:\")\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cnf_matrix = confusion_matrix(actual, pred_outcome_00_thresh)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['No','Yes'],\n",
    "                        title='Confusion matrix, without normalization')\n",
    "    print(\"Metrics at 0.0 thresh thresh (specificity is recall for negative class):\")\n",
    "    print(classification_report(actual, pred_outcome_00_thresh, target_names=['No','Yes']))\n",
    "\n",
    "    # # plot threshold vs ppv curve\n",
    "    plt.plot(thresholds, precision[0:len(precision)-1], color='b')\n",
    "\n",
    "    plt.xlabel('Threshold probability')\n",
    "    plt.ylabel('Precision (PPV)')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Threshold vs precision')\n",
    "    plt.show()\n",
    "\n",
    "    # histogram\n",
    "    plt.hist(predicted)\n",
    "    plt.title(\"Histogram\")\n",
    "    plt.xlabel(\"Predicted probability\" )\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b02066",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(val_logits_list[:, 1], val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(test_logits_list[:, 1], test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
