{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a63e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, concatenate, Average\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, TimeDistributed, GRU, Bidirectional, Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16837d05",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "with open('notes_tokenizer_ps_find.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_clinical_notes (Valid PS).csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = data[\"text_no_ps\"].tolist()\n",
    "text_len_list = [len(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863adcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data[\"split\"] == \"train\"]\n",
    "valid_data = data[data[\"split\"] == \"validation\"]\n",
    "test_data = data[data[\"split\"] == \"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_note_length = 2000\n",
    "\n",
    "x_text_train = sequence.pad_sequences(tokenizer.texts_to_sequences([str(x) for x in train_data['text_no_ps']]), maxlen=max_note_length, padding='post')\n",
    "x_text_valid = sequence.pad_sequences(tokenizer.texts_to_sequences([str(x) for x in valid_data['text_no_ps']]), maxlen=max_note_length, padding='post')\n",
    "x_text_test = sequence.pad_sequences(tokenizer.texts_to_sequences([str(x) for x in test_data['text_no_ps']]), maxlen=max_note_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64443a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_model(target):\n",
    "    vocab_size = 10000\n",
    "    embedding_dims = 256\n",
    "    filters = 250\n",
    "    kernel_size = 3\n",
    "    epochs = 2\n",
    "    hidden_dims = 128\n",
    "    max_note_length=2000\n",
    "    batch_size = 32\n",
    "\n",
    "    # make model\n",
    "    text_input = Input(shape=(max_note_length,), dtype='float32')\n",
    "\n",
    "    text_embed = Embedding(vocab_size, embedding_dims, input_length=max_note_length, mask_zero=False)(text_input)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=500, kernel_size=kernel_size, strides=1, padding='valid')(text_embed)\n",
    "    x = GlobalMaxPooling1D()(cnn1)\n",
    "\n",
    "    hidden = Dense(hidden_dims)(x)\n",
    "    hidden = Activation('relu')(hidden)\n",
    "\n",
    "    output = Dense(1, activation='linear')(hidden)\n",
    "\n",
    "    model = Model(inputs=text_input, outputs=output)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea2af1",
   "metadata": {},
   "source": [
    "## Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an unnecessary loop with only 1 element 'ps', kept here for convenience (old code)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='ps_high'+'.h5',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss'),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=1e-2,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "\n",
    "this_model = get_simple_model('ps_high')\n",
    "this_model.fit(x_text_train, train_data['ps_high'].values,\n",
    "         validation_data=(x_text_valid, valid_data['ps_high'].values),\n",
    "         epochs=100,\n",
    "         batch_size = 32, use_multiprocessing=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c726d0",
   "metadata": {},
   "source": [
    "## Validation and test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_model = get_simple_model('ps_high')\n",
    "this_model.load_weights('ps_high'+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to plot model metrics\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "  \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52756aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model auc\n",
    "def eval_model(predicted, actual):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import auc\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    print(\"AUC \" + str(roc_auc_score(actual, predicted)))\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    fpr, tpr, threshold = roc_curve(actual, predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    average_precision = average_precision_score(actual, predicted)\n",
    "\n",
    "    print('Average precision score: {0:0.2f}'.format(\n",
    "        average_precision))\n",
    "\n",
    "\n",
    "    # method I: plt\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.title('Receiver Operating Characteristic: ' )\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    #from sklearn.utils.fixes import signature\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(actual, predicted)\n",
    "    \n",
    "    outcome_counts = np.unique(actual, return_counts=True)[1]\n",
    "    prob_outcome = outcome_counts[1] / (outcome_counts[0] + outcome_counts[1])\n",
    "    print('Outcome probability:')\n",
    "    print(prob_outcome)\n",
    "    \n",
    "    plt.plot(recall, precision, color='b')\n",
    "    plt.plot([0,1],[prob_outcome,prob_outcome], 'r--')\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b')\n",
    "\n",
    "    plt.xlabel('Recall (Sensitivity)')\n",
    "    plt.ylabel('Precision (PPV)')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "            average_precision))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # best F1\n",
    "    F1 = 2*((precision*recall)/(precision+recall))\n",
    "    print(\"Best F1 \")\n",
    "    print(max(F1))\n",
    "    \n",
    "    # threshold for best F1\n",
    "    bestF1_thresh = thresholds[np.argmax(F1)]\n",
    "    print(\"Threshold for best F1:\")\n",
    "    print(bestF1_thresh)\n",
    "    pred_outcome_best_f1_thresh = np.where(predicted >= bestF1_thresh,1,0)\n",
    "    print(np.unique(pred_outcome_best_f1_thresh, return_counts=True))\n",
    "    pred_outcome_00_thresh = np.where(predicted >= 0.0,1,0)\n",
    "    \n",
    "    # # predictions\n",
    "    \n",
    "    # # confusion matrix\n",
    "    print(\"Confusion matrix at best F1 thresh:\")\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cnf_matrix = confusion_matrix(actual, pred_outcome_best_f1_thresh)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['No','Yes'],\n",
    "                        title='Confusion matrix, without normalization')\n",
    "    print(\"Metrics at best F1 thresh (specificity is recall for negative class):\")\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(actual, pred_outcome_best_f1_thresh, target_names=['No','Yes']))\n",
    "\n",
    "\n",
    "    print(\"Confusion matrix at 0.0 thresh:\")\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cnf_matrix = confusion_matrix(actual, pred_outcome_00_thresh)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['No','Yes'],\n",
    "                        title='Confusion matrix, without normalization')\n",
    "    print(\"Metrics at 0.0 thresh thresh (specificity is recall for negative class):\")\n",
    "    print(classification_report(actual, pred_outcome_00_thresh, target_names=['No','Yes']))\n",
    "\n",
    "    # # plot threshold vs ppv curve\n",
    "    plt.plot(thresholds, precision[0:len(precision)-1], color='b')\n",
    "\n",
    "    plt.xlabel('Threshold probability')\n",
    "    plt.ylabel('Precision (PPV)')\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Threshold vs precision')\n",
    "    plt.show()\n",
    "\n",
    "    # histogram\n",
    "    plt.hist(predicted)\n",
    "    plt.title(\"Histogram\")\n",
    "    plt.xlabel(\"Predicted probability\" )\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_logits_list = this_model.predict(x_text_valid)\n",
    "eval_model(val_logits_list, valid_data['high_ps'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits_list = this_model.predict(x_text_test)\n",
    "eval_model(test_logits_list, test_data['high_ps'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
